{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "sys.path.insert(0,'..')\r\n",
    "import pandas as pd\r\n",
    "from lib_template.settings import DATA_DIR\r\n",
    "import glob\r\n",
    "import pathlib\r\n",
    "import joblib\r\n",
    "import pickle\r\n",
    "\r\n",
    "\r\n",
    "def _process_data(DATA_DIR, temporary_dir= \"temporary\", processed_dir= \"persistent\", file_type= \"csv\"):\r\n",
    "    file_list = glob.glob(f'{DATA_DIR}\\\\{temporary_dir}\\\\**', recursive=True)\r\n",
    "    file_of_matched_type = [file for file in file_list if file.endswith(f\".{file_type}\")]\r\n",
    "    matched_dfs = [pd.read_csv(file) for file in file_of_matched_type]\r\n",
    "    file_hash = [joblib.hash(df, hash_name='sha1') for df in matched_dfs]\r\n",
    "    file_names = [str(pathlib.Path(f).name).replace(f\".{file_type}\",\"\") for f in file_of_matched_type]\r\n",
    "    new_files_collector = []\r\n",
    "    for hash_key, old_file in zip(file_hash,file_of_matched_type):\r\n",
    "        new_file_name = f\"{hash_key}.{file_type}\"\r\n",
    "        new_file = pathlib.Path(os.path.join(DATA_DIR, processed_dir, new_file_name))\r\n",
    "        old_file_path = pathlib.Path(old_file)\r\n",
    "        if not new_file.is_file():\r\n",
    "            old_file_path.rename(new_file)\r\n",
    "        old_file_path.unlink(missing_ok = True)\r\n",
    "        new_files_collector.append(new_file)\r\n",
    "    data_lock = dict(zip(file_hash, new_files_collector))\r\n",
    "    data_name_dict = dict(zip(file_hash, file_names))\r\n",
    "    return data_lock, data_name_dict\r\n",
    "\r\n",
    "\r\n",
    "def _generate_data_lock(DATA_DIR, data_hash_lock=\"local_data_lock.pickle\", data_name_metadata = \"local_meta_data.pickle\"):\r\n",
    "    new_data_lock, data_name_dict = _process_data(DATA_DIR)\r\n",
    "    data_lock_path = pathlib.Path(os.path.join(DATA_DIR, data_hash_lock))\r\n",
    "    meta_data_path = pathlib.Path(os.path.join(DATA_DIR, data_name_metadata))\r\n",
    "    if data_lock_path.is_file():\r\n",
    "        with open(data_lock_path, 'rb') as handle:\r\n",
    "            old_data_lock = pickle.load(handle)\r\n",
    "        if new_data_lock:\r\n",
    "            data_lock = {**old_data_lock, **new_data_lock}\r\n",
    "        else:\r\n",
    "            data_lock = old_data_lock.copy()\r\n",
    "    else:\r\n",
    "        data_lock = new_data_lock.copy()\r\n",
    "    if data_lock:\r\n",
    "        with open(data_lock_path, 'wb') as handle:\r\n",
    "            pickle.dump(data_lock, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "    return data_lock, data_lock_path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "data_lock, data_lock_path = _generate_data_lock(DATA_DIR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "source": [
    "data_lock_path"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "WindowsPath('C:/Workspace/projects/lib_template/data/local_data_lock.pickle')"
      ]
     },
     "metadata": {},
     "execution_count": 202
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "data_lock"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'090ade65772b70f36184e84745715a0d6c277982': WindowsPath('C:/Workspace/projects/lib_template/data/persistent/090ade65772b70f36184e84745715a0d6c277982.csv'),\n",
       " 'c0e894ee39909eb2cc05fd9f597b016a908fcb5d': WindowsPath('C:/Workspace/projects/lib_template/data/persistent/c0e894ee39909eb2cc05fd9f597b016a908fcb5d.csv')}"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "# get list of files present in persistent folder in blob.\r\n",
    "# whatever is not matching with the local data_lock file upload that\r\n",
    "# update the online data lock file\r\n",
    "\r\n",
    "\r\n",
    "# read the file present in presistent folder. extract hash key of each file.\r\n",
    "# read data lock file in local. check diff, download only the files which are not present in local.\r\n",
    "# update the local data lock file."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}